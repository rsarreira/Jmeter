FaillAllTestsOnError=true

# Used in Hadoop Job Name field of Map Reduce jobs. Please modify "user:automation" and enter your name.
mapreduce_description=OS:${os.name} version:${Internal.Kettle.Version} user:automation

kinit_user=devuser
shim_active=mapr401
secure_cluster=false

# HDFS
#hdfsProto=hdfs
hdfsProto=maprfs
hdfsServer=bad-mapr401unsec-cent-n1.pentaho.dmz
hdfsPort=8020
hdfsWorkDir=/user/devuser/test2
hdfsUrl=${hdfsProto}:///
useChmod777=true
sshServer=bad-mapr401unsec-cent-n1.pentaho.dmz
sshUser=mapr
sshPassword=pentaho06
kinitPassword=password

# Job Tracker/Resource Manager
jobTrackerServer=bad-mapr401unsec-cent-n3.pentaho.dmz
jobTrackerPort=8032

# Test Mysql DB 
mysql_hostname=10.177.176.105
mysql_dbname=test
mysql_username=root
mysql_password=pentaho06
mysql_port=3306
mysql_driver=com.mysql.jdbc.Driver

# Hive2 Connection
hive2_hostname=bad-mapr401unsec-cent-n1.pentaho.dmz
hive2_user=devuser
hive2_password=devuser
hive2_port=10000
hive2_db=default
hive2_option=
hive2_principal=
hive2_tablename=weblogs

# Hbase table names
hbase_weblogs=hbweblogs
hbase_weblogs2=hbweblogs2

# Zookeper
zookeeper_host=bad-mapr401unsec-cent-n1.pentaho.dmz
zookeeper_port=5181

# Oozie
oozie_server=bad-mapr401unsec-cent-n1.pentaho.dmz
oozie_url=http://${oozie_server}:11000/oozie/
oozie_outputDir=map-reduce
#oozie_oozie.wf.application.path=/user/pdi/examples/apps/map-reduce /bad-hdp21sec-cent-mn.pentaho.com@HDP21SEC.PENTAHO.COM
oozie_queueName=default
oozie_user.name=mapr
jobTracker=${jobTrackerServer}:${jobTrackerPort}


# Sqoop
sqoop_user.name=sqoop
#sqoop_connect=jdbc:mysql://${mysql_hostname}:${mysql_port}/${mysql_dbname}?defaultFetchSize=500&useCursorFetch=true

# Sqoop - hive
sqoop_hive_inputTable=aggregatehdfs
sqoop_hive_outputTable=hivesqooptest3
sqoop_split-by=client_ip
sqoop_query=

# Sqoop - hdfs
sqoop_hdfs_inputTable=testSqoop
sqoop_hdfsExportTable=aggregatehdfsExport

# Sqoop - hbase
sqoop_hbase_inputTable=aggregatehdfs
sqoop_hbase_outputTable=hbasesqooptest2
sqoop_hbase_column-family=client_ip
sqoop_secure_libjar_path=

# Yarn 
yarn_default_fs=${hdfsProto}:///

# Spark
spark_submit_utility=/home/devuser/share/spark_client/spark-1.6.2-bin-hadoop2.6/bin/spark-submit
spark_yarn_jar=${hdfsUrl}/user/spark/spark-assembly.jar
spark_driver_extraJavaOptions=
spark_yarn_am_extraJavaOptions=
