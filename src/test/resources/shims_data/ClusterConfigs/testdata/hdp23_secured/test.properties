FaillAllTestsOnError=true

# Used in Hadoop Job Name field of Map Reduce jobs. Please modify "user:automation" and enter your name.
mapreduce_description=OS:${os.name} version:${Internal.Kettle.Version} user:automation

kinit_user=devuser
shim_active=hdp23
secure_cluster=true

# HDFS
hdfsProto=hdfs
#hdfsProto=maprfs
hdfsServer=hdp23secure
hdfsPort=
hdfsWorkDir=/user/devuser/test2
hdfsUrl=${hdfsProto}://${hdfsServer}
useChmod777=true
sshServer=svqxbdcn6hdp23secn1.pentahoqa.com
sshUser=devuser
sshPassword=pentaho06
kinitPassword=password

# Job Tracker/Resource Manager
jobTrackerServer=svqxbdcn6hdp23secn2.pentahoqa.com
jobTrackerPort=8050

# Test Mysql DB 
mysql_hostname=10.177.176.105
mysql_dbname=test
mysql_username=root
mysql_password=pentaho06
mysql_port=3306
mysql_driver=com.mysql.jdbc.Driver

# Hive2 Connection
hive2_hostname=svqxbdcn6hdp23secn1.pentahoqa.com
hive2_user=devuser
hive2_password=devuser
hive2_port=10000
hive2_db=default
hive2_option=principal
hive2_principal=hive/svqxbdcn6hdp23secn1.pentahoqa.com@PENTAHOQA.COM
hive2_tablename=weblogs

# Hbase table names
hbase_weblogs=hbweblogs
hbase_weblogs2=hbweblogs2

# Zookeper
zookeeper_host=svqxbdcn6hdp23secn1.pentahoqa.com
zookeeper_port=2181

# Oozie
oozie_server=svqxbdcn6hdp23secn3.pentahoqa.com
oozie_url=http://${oozie_server}:11000/oozie/
oozie_outputDir=map-reduce
#oozie_oozie.wf.application.path=/user/pdi/examples/apps/map-reduce /bad-hdp21sec-cent-mn.pentaho.com@HDP21SEC.PENTAHO.COM
oozie_queueName=default
oozie_user.name=devuser


# Sqoop
sqoop_user.name=sqoop
#sqoop_connect=jdbc:mysql://${mysql_hostname}:${mysql_port}/${mysql_dbname}?defaultFetchSize=500&useCursorFetch=true

# Sqoop - hive
sqoop_hive_inputTable=aggregatehdfs
sqoop_hive_outputTable=hivesqooptest3
sqoop_split-by=client_ip
sqoop_query=

# Sqoop - hdfs
sqoop_hdfs_inputTable=testSqoop
sqoop_hdfsExportTable=aggregatehdfsExport

# Sqoop - hbase
sqoop_hbase_inputTable=aggregatehdfs
sqoop_hbase_outputTable=hbasesqooptest2
sqoop_hbase_column-family=client_ip
sqoop_secure_libjar_path=/home/devuser/pentaho/design-tools/data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23/lib/pentaho-hadoop-shims-hdp23-security-61.2016.00.00-85.jar

# Yarn 
yarn_default_fs=${hdfsProto}://${hdfsServer}

# Spark
spark_submit_utility=/media/sf_Builds/Spark/spark-1.3.1-bin-hadoop2.3/bin/spark-submit
spark_yarn_jar=${hdfsUrl}/user/spark/spark-assembly-1.3.1.2.3.0.0-2557-hadoop2.7.1.2.3.0.0-2557.jar
spark_driver_extraJavaOptions=-Dhdp.version=2.3.0.0-2557
spark_yarn_am_extraJavaOptions=-Dhdp.version=2.3.0.0-2557
