FaillAllTestsOnError=true

# Used in Hadoop Job Name field of Map Reduce jobs. Please modify "user:automation" and enter your name.
mapreduce_description=OS:${os.name} version:${Internal.Kettle.Version} user:automation

kinit_user=devuser
shim_active=hdp24
secure_cluster=true

# HDFS
hdfsProto=hdfs
#hdfsProto=maprfs
hdfsServer=hdp24secure
hdfsPort=
hdfsWorkDir=/user/devuser/test2
hdfsUrl=${hdfsProto}://${hdfsServer}
useChmod777=true
sshServer=svqxbdcn6hdp24secn1.pentahoqa.com
sshUser=devuser
sshPassword=password
kinitPassword=password

# Job Tracker/Resource Manager
jobTrackerServer=svqxbdcn6hdp24secn3.pentahoqa.com
jobTrackerPort=8050

# Test Mysql DB 
mysql_hostname=10.177.176.105
mysql_dbname=test
mysql_username=root
mysql_password=pentaho06
mysql_port=3306
mysql_driver=com.mysql.jdbc.Driver

# Hive2 Connection
hive2_hostname=svqxbdcn6hdp24secn2.pentahoqa.com
hive2_user=devuser
hive2_password=devuser
hive2_port=10000
hive2_db=default
hive2_option=principal
hive2_principal=hive/svqxbdcn6hdp24secn2.pentahoqa.com@PENTAHOQA.COM
hive2_tablename=weblogs

# Hbase table names
hbase_weblogs=hbweblogs
hbase_weblogs2=hbweblogs2

# Zookeper
zookeeper_host=svqxbdcn6hdp24secn1.pentahoqa.com,svqxbdcn6hdp24secn2.pentahoqa.com,svqxbdcn6hdp24secn3.pentahoqa.com
zookeeper_port=2181

# Oozie
oozie_server=svqxbdcn6hdp24secn2.pentahoqa.com
oozie_url=http://${oozie_server}:11000/oozie/
oozie_outputDir=map-reduce
#oozie_oozie.wf.application.path=/user/pdi/examples/apps/map-reduce /svqxbdcn6hdp24secn1.pentahoqa.com@HDP21SEC.PENTAHO.COM
oozie_queueName=default
oozie_user.name=devuser
jobTracker=${jobTrackerServer}:${jobTrackerPort}

# Sqoop
sqoop_user.name=sqoop
#sqoop_connect=jdbc:mysql://${mysql_hostname}:${mysql_port}/${mysql_dbname}?defaultFetchSize=500&useCursorFetch=true

# Sqoop - hive
sqoop_hive_inputTable=aggregatehdfs
sqoop_hive_outputTable=hivesqooptest3
sqoop_split-by=client_ip
sqoop_query=

# Sqoop - hdfs
sqoop_hdfs_inputTable=testSqoop
sqoop_hdfsExportTable=aggregatehdfsExport

# Sqoop - hbase
sqoop_hbase_inputTable=aggregatehdfs
sqoop_hbase_outputTable=hbasesqooptest2
sqoop_hbase_column-family=client_ip
sqoop_secure_libjar_path=/opt/Pentaho_6101_196/design-tools/data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp24/lib/pentaho-hadoop-shims-hdp24-security-61.2016.05.00-201.jar

# Yarn 
yarn_default_fs=${hdfsProto}://${hdfsServer}

# Spark
spark_submit_utility=/home/devuser/share/spark_client/spark-1.6.2-bin-hadoop2.6/bin/spark-submit
spark_yarn_jar=${hdfsUrl}/user/spark/spark-assembly-1.6.0.2.4.0.0-169-hadoop2.7.1.2.4.0.0-169.jar
spark_driver_extraJavaOptions=-Dhdp.version=2.4.0.0-169
spark_yarn_am_extraJavaOptions=-Dhdp.version=2.4.0.0-169
