FaillAllTestsOnError=true

# Used in Hadoop Job Name field of Map Reduce jobs. Please modify "user:automation" and enter your name.
mapreduce_description=OS:${os.name} version:${Internal.Kettle.Version} user:automation

kinit_user=devuser
shim_active=cdh55
secure_cluster=true

# HDFS
hdfsProto=hdfs
#hdfsProto=maprfs
hdfsServer=cdh55secure
hdfsPort=
hdfsWorkDir=/user/devuser/test2
hdfsUrl=${hdfsProto}://${hdfsServer}
useChmod777=true
sshServer=svqxbdcn6cdh55secn1.pentahoqa.com
sshUser=devuser
sshPassword=password
kinitPassword=password

# Job Tracker/Resource Manager
jobTrackerServer=svqxbdcn6cdh55secn3.pentahoqa.com
jobTrackerPort=8032

# Test Mysql DB 
mysql_hostname=10.177.176.105
mysql_dbname=test
mysql_username=root
mysql_password=pentaho06
mysql_port=3306
mysql_driver=com.mysql.jdbc.Driver

# Hive2 Connection
hive2_hostname=svqxbdcn6cdh55secn1.pentahoqa.com
hive2_user=devuser
hive2_password=devuser
hive2_port=10000
hive2_db=default
hive2_option=principal
hive2_principal=hive/svqxbdcn6cdh55secn1.pentahoqa.com@pentahoqa.com
hive2_tablename=weblogs

# Impala Connection
impala_hostname=svqxbdcn6cdh55secn1.pentahoqa.com
impala_user=
impala_password=
impala_port=21050
impala_db=default
impala_KrbRealm=PENTAHOQA.COM
impala_KrbHostFQDN=svqxbdcn6cdh55secn1.pentahoqa.com
impala_KrbServiceName=impala
impala_tablename=weblogsimp

# Hbase table names
hbase_weblogs=hbweblogs
hbase_weblogs2=hbweblogs2

# Zookeper
zookeeper_host=svqxbdcn6cdh55secn1.pentahoqa.com,svqxbdcn6cdh55secn2.pentahoqa.com,svqxbdcn6cdh55secn3.pentahoqa.com
zookeeper_port=2181

# Oozie
oozie_server=svqxbdcn6cdh55secn1.pentahoqa.com
oozie_url=http://${oozie_server}:11000/oozie/
oozie_outputDir=map-reduce
#oozie_oozie.wf.application.path=/user/pdi/examples/apps/map-reduce /bad-hdp21sec-cent-mn.pentaho.com@HDP21SEC.PENTAHO.COM
oozie_queueName=default
oozie_user.name=devuser
jobTracker=${jobTrackerServer}:${jobTrackerPort}


# Sqoop
sqoop_user.name=sqoop
#sqoop_connect=jdbc:mysql://${mysql_hostname}:${mysql_port}/${mysql_dbname}?defaultFetchSize=500&useCursorFetch=true

# Sqoop - hive
sqoop_hive_inputTable=aggregatehdfs
sqoop_hive_outputTable=hivesqooptest3
sqoop_split-by=client_ip
sqoop_query=

# Sqoop - hdfs
sqoop_hdfs_inputTable=testSqoop
sqoop_hdfsExportTable=aggregatehdfsExport

# Sqoop - hbase
sqoop_hbase_inputTable=aggregatehdfs
sqoop_hbase_outputTable=hbasesqooptest2
sqoop_hbase_column-family=client_ip
sqoop_secure_libjar_path=/home/devuser/Pentaho60N_366/design-tools/data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/cdh55/lib/pentaho-hadoop-shims-cdh55-security-6.0-NIGHTLY-366.jar

# Yarn 
yarn_default_fs=${hdfsProto}://${hdfsServer}

# Spark
spark_submit_utility=/home/devuser/share/spark_client/spark-1.6.2-bin-hadoop2.6/bin/spark-submit
spark_yarn_jar=${hdfsUrl}/user/spark/spark-assembly-1.5.0-cdh5.5.0-hadoop2.6.0-cdh5.5.0.jar
spark_driver_extraJavaOptions=
spark_yarn_am_extraJavaOptions=
