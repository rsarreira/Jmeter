FaillAllTestsOnError=true

# Used in Hadoop Job Name field of Map Reduce jobs. Please modify "user:automation" and enter your name.
mapreduce_description=OS:${os.name} version:${Internal.Kettle.Version} user:automation

kinit_user=devuser
shim_active=hdp22
secure_cluster=true

# HDFS
hdfsProto=hdfs
#hdfsProto=maprfs
hdfsServer=hdp22secure
hdfsPort=
hdfsWorkDir=/user/devuser/test2
hdfsUrl=${hdfsProto}://${hdfsServer}
useChmod777=true
sshServer=bad-hdp22-cent-n1.pentaho.dmz
sshUser=devuser
sshPassword=pentaho06
kinitPassword=password

# Job Tracker/Resource Manager
jobTrackerServer=bad-hdp22-cent-n2.pentaho.dmz
jobTrackerPort=8050

# Test Mysql DB 
mysql_hostname=10.177.176.105
mysql_dbname=test
mysql_username=root
mysql_password=pentaho06
mysql_port=3306
mysql_driver=com.mysql.jdbc.Driver

# Hive2 Connection
hive2_hostname=bad-hdp22-cent-n1.pentaho.dmz
hive2_user=devuser
hive2_password=devuser
hive2_port=10000
hive2_db=default
hive2_option=principal
hive2_principal=hive/bad-hdp22-cent-n1.pentaho.dmz@PENTAHO.DMZ
hive2_tablename=weblogs

# Hbase table names
hbase_weblogs=hbweblogs
hbase_weblogs2=hbweblogs2

# Zookeper
zookeeper_host=bad-hdp22-cent-n2.pentaho.dmz
zookeeper_port=2181

# Oozie
oozie_server=bad-hdp22-cent-n1.pentaho.dmz
oozie_url=http://${oozie_server}:11000/oozie/
oozie_outputDir=map-reduce
#oozie_oozie.wf.application.path=/user/pdi/examples/apps/map-reduce /bad-hdp21sec-cent-mn.pentaho.com@HDP21SEC.PENTAHO.COM
oozie_queueName=default
oozie_user.name=devuser
jobTracker=${jobTrackerServer}:${jobTrackerPort}


# Sqoop
sqoop_user.name=sqoop
#sqoop_connect=jdbc:mysql://${mysql_hostname}:${mysql_port}/${mysql_dbname}?defaultFetchSize=500&useCursorFetch=true

# Sqoop - hive
sqoop_hive_inputTable=aggregatehdfs
sqoop_hive_outputTable=hivesqooptest3
sqoop_split-by=client_ip
sqoop_query=

# Sqoop - hdfs
sqoop_hdfs_inputTable=testSqoop
sqoop_hdfsExportTable=aggregatehdfsExport

# Sqoop - hbase
sqoop_hbase_inputTable=aggregatehdfs
sqoop_hbase_outputTable=hbasesqooptest2
sqoop_hbase_column-family=client_ip
sqoop_secure_libjar_path=/home/devuser/pentaho/design-tools/data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp22/lib/pentaho-hadoop-shims-hdp22-security-5.4-NIGHTLY.jar


# Yarn 
yarn_default_fs=${hdfsProto}://${hdfsServer}

# Spark
spark_submit_utility=/home/devuser/share/spark_client/spark-1.3.1-bin-hadoop2.3/bin/spark-submit
spark_yarn_jar=${hdfsUrl}/user/spark/spark-assembly.jar
spark_driver_extraJavaOptions=
spark_yarn_am_extraJavaOptions=